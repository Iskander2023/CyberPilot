//
//  VoiceControlManager.swift
//  CyberPilot
//
//  Created by Aleksandr Chumakov on 1/07/25.
//
import Foundation
import AVFoundation
import Speech
import Combine


final class VoiceService: NSObject, ObservableObject {
    @Published var transcribedText: String = ""
    @Published var isListening: Bool = false
    @Published var isSpeaking: Bool = false
    @Published var voiceControlShouldStop: Bool = false

    
    let commandSender: CommandSender
    let logger = CustomLogger(logLevel: .info, includeMetadata: false)

    private var speechRecognizer: SFSpeechRecognizer?
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let synthesizer = AVSpeechSynthesizer()
    private var silenceTimer: Timer?
    private var lastProcessedCommandText: String = ""
    private var repeatCommandTimer: Timer?
    private var lastDirection: [String: Bool]?
    private var isVoiceControlStopped = false
    
    init(commandSender: CommandSender) {
            self.commandSender = commandSender
            super.init()
            // —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –≤—Ö–æ–¥—è—â–µ–º –∑–≤–æ–Ω–∫–µ
            NotificationCenter.default.addObserver(
                    self,
                    selector: #selector(handleAudioSessionInterruption),
                    name: AVAudioSession.interruptionNotification,
                    object: nil
                )
            // —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Å–º–µ–Ω–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            NotificationCenter.default.addObserver(
                self,
                selector: #selector(handleRouteChange),
                name: AVAudioSession.routeChangeNotification,
                object: nil
            )
        }
    
    
    @objc private func handleAudioSessionInterruption(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }

        if type == .began {
            logger.info("üîá –ê—É–¥–∏–æ—Å–µ—Å—Å–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞")
            stopListening()
        }
        // –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞ –ø–æ—Å–ª–µ –∑–≤–æ–Ω–∫–∞
//        if type == .ended,
//           let optionsValue = userInfo[AVAudioSessionInterruptionOptionKey] as? UInt {
//            let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
//            if options.contains(.shouldResume) {
//                try? startListening()
//            }
//        }

    }
    
    
    @objc private func handleRouteChange(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let reasonValue = userInfo[AVAudioSessionRouteChangeReasonKey] as? UInt,
              let reason = AVAudioSession.RouteChangeReason(rawValue: reasonValue) else {
            return
        }
        switch reason {
        case .newDeviceAvailable:
            logger.info("üéß –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –Ω–æ–≤–æ–µ –∞—É–¥–∏–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ")
            restartAudioSession()
        case .oldDeviceUnavailable:
            logger.info("üîå –ê—É–¥–∏–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            restartAudioSession()
        default:
            break
        }
    }
    
    // –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∞—É–¥–∏–æ —Å–µ—Å—Å–∏–∏
    func restartAudioSession() {
        stopListening()
        settingAudioSession()
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            self?.startVoiceControl()
        }
    }
    
    // —Ç–∞–π–º–µ—Ä –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ 30 —Å–µ–∫—É–Ω–¥
    func restartSilenceTimer() {
        silenceTimer?.invalidate()
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 30.0, repeats: false) { [weak self] _ in
            self?.logger.info("‚èπ –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: —Ç–∏—à–∏–Ω–∞")
            self?.stopListening()
        }
    }
    
    
    func requestAuthorization() {
        
        if speechRecognizer?.supportsOnDeviceRecognition == true {
            recognitionRequest?.requiresOnDeviceRecognition = true
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ")
        } else {
            logger.info("–õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
        }
        // –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        self.speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: AppConfig.VoiceService.language))
            if speechRecognizer == nil {
                logger.info("‚õîÔ∏è –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
                return
            }
        // —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    self.logger.info("‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ.")
                default:
                    self.logger.info("‚õîÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ: \(authStatus)")
                }
            }
        }
        // –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É
        if #available(iOS 17.0, *) {
            AVAudioApplication.requestRecordPermission { granted in
                if !granted {
                    self.logger.info("–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω")
                }
            }
        } else {
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                if !granted {
                    self.logger.info("–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω")
                }
            }
        }
    }
    
    
    // –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ–ª–æ—Å–æ–≤ ru-RU - –º–∏–ª–µ–Ω–∞ —Ç–µ–∫—É—â–∏–π
    func checkVoices() {
        let voices = AVSpeechSynthesisVoice.speechVoices()
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –≥–æ–ª–æ—Å–∞: \(voices)")
    }
    
    
    // –û—Å—Ç–∞–Ω–∞–≤–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á –∞—É–¥–∏–æ–¥–≤–∏–∂–∫–∞
    func stopAudioEngine() {
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
    }
    
    
    // –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è –∞—É–¥–∏–æ–¥–≤–∏–∂–∫–∞
    func deactivateAudioEngine() {
        do {
            // –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–µ—Å—Å–∏–∏ –ø–µ—Ä–µ–¥ –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π
            if AVAudioSession.sharedInstance().isOtherAudioPlaying {
                try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
                logger.info("‚úÖ –ê—É–¥–∏–æ—Å–µ—Å—Å–∏—è —É—Å–ø–µ—à–Ω–æ –¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞")
            }
        } catch {
            logger.error("üõë –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏: \(error.localizedDescription)")
            try? AVAudioSession.sharedInstance().setActive(false, options: [])
        }
    }
    
    
    // –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    func stopVoiceControl() {
        logger.info("üõë –í—ã–∑–≤–∞–Ω stopVoiceControl")
        stopListening()
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                self.deactivateAudioEngine()
            }
        stopRepeatingCommand()
        voiceControlShouldStop = true
    }
    
    
    // –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    func stopListening() {
        guard isListening else { return }
        logger.info("üõë stopListening")
        stopAudioEngine()
        isListening = false
        recognitionRequest = nil
        recognitionTask = nil
        silenceTimer?.invalidate()
        silenceTimer = nil
    }
    
    
    // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏
    func settingAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord,
                                         mode: .voiceChat,
                                         options: [.duckOthers, .defaultToSpeaker, .allowBluetooth])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            logger.info("‚úÖ –ê—É–¥–∏–æ—Å–µ—Å—Å–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
        } catch {
            logger.info("‚õîÔ∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏: \(error.localizedDescription)")
            return
        }
    }
    
    
    // –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ–¥–≤–∏–∂–æ–∫
    func startAudioEngine() {
        do {
            audioEngine.prepare()
            try audioEngine.start()
            logger.info("‚úÖ AudioEngine –∑–∞–ø—É—â–µ–Ω, –º–∏–∫—Ä–æ—Ñ–æ–Ω –∞–∫—Ç–∏–≤–µ–Ω")
        } catch {
            logger.info("‚õîÔ∏è –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ audioEngine: \(error.localizedDescription)")
            return
        }
    }

    
    func startVoiceControl() {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        guard let speechRecognizer = speechRecognizer else {
            logger.info("‚õîÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –¥–ª—è \(AppConfig.VoiceService.language) –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
            return
        }
        guard !audioEngine.isRunning else {
            logger.info("‚ö†Ô∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ")
            return
        }
        isListening = true
        
        settingAudioSession()
        
        // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            logger.info("‚õîÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å recognitionRequest")
            return
        }
        
        // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω–Ω–æ–≥–æ –≤—Ö–æ–¥–∞
        let inputNode = audioEngine.inputNode
        inputNode.removeTap(onBus: 0) // –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π tap, –µ—Å–ª–∏ –±—ã–ª
        let inputFormat = inputNode.inputFormat(forBus: 0)
        logger.info("üì¢ –§–æ—Ä–º–∞—Ç –∞—É–¥–∏–æ: sampleRate=\(inputFormat.sampleRate), channels=\(inputFormat.channelCount)")
        inputNode.installTap(onBus: 0, bufferSize: 2048, format: inputFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        startAudioEngine()
        
        // –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –±—É—Ñ–µ—Ä
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }

            if let error = error {
                self.logger.info("‚õîÔ∏è –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: \(error.localizedDescription)")
                //self.stopListening()
                return
            }

            guard let result = result else { return }

            let text = result.bestTranscription.formattedString
            self.transcribedText = text
            self.restartSilenceTimer()

            
            let words = text.lowercased().components(separatedBy: " ").filter { !$0.isEmpty }
            let lastWord = words.suffix(3).joined(separator: " ")  // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1‚Äì2 —Å–ª–æ–≤–∞
//            let lastWord = words.last ?? "" // –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
            
            guard lastWord != self.lastProcessedCommandText else { return }
            
            if let voiceCommand = VoiceCommand.parse(from: lastWord) {
                switch voiceCommand.category {
                case .movement:
                    let direction = self.commandDirection(from: voiceCommand)
                    self.lastDirection = direction
                    self.startRepeatingLastCommand() // –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∫–æ–º–∞–Ω–¥—ã
                    self.logger.info("‚úÖ –ö–æ–º–∞–Ω–¥–∞: \(direction)")
                    self.logger.info("üì¢ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: \(lastWord)")
                    self.transcribedText = ""
                    self.lastProcessedCommandText = lastWord // ‚úÖ –ó–∞–ø–æ–º–∏–Ω–∞–µ–º
                case .system:
                    self.commandSystem(from: voiceCommand)// –≤–∞—Ä–∏–∞–Ω—Ç –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã –±–µ–∑ —Å–ø–∞–º–∞
                    self.transcribedText = ""
                }
                
            } else {
                self.logger.info("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: \(lastWord)")
            }

            if result.isFinal {
                self.logger.info("‚èπ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                self.lastDirection = nil
            }
        }
    }
    
    // –∫–æ–º–∞–Ω–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º
    func commandSystem(from command: VoiceCommand?) {
        if let sys = command {
            switch sys {
            case .stopVoiceControl:
//                speak(text: AppConfig.VoiceControl.stopVoiceMessage) {
//                    self.stopVoiceControl()
//                }
                self.stopVoiceControl()
            default:
                    break
            }
        }
    }
    
    // –∫–æ–º–∞–Ω–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º —Ä–æ–±–æ—Ç–∞
    func commandDirection(from command: VoiceCommand?) -> [String: Bool] {
        var flags = [
            "forward": false,
            "backward": false,
            "left": false,
            "right": false
        ]
        if let cmd = command {
            switch cmd {
            case .forward:
                flags["forward"] = true
            case .backward:
                flags["backward"] = true
            case .left:
                flags["left"] = true
            case .right:
                flags["right"] = true
            case .forwardLeft:
                flags["forward"] = true
                flags["left"] = true
            case .forwardRight:
                flags["forward"] = true
                flags["right"] = true
            case .stop:
                self.lastDirection = nil
                stopRepeatingCommand()
            default:
                    break
            }
        }

        return flags
    }
    
    
    // –æ—Ç–ø—Ä–∞–≤–∫–∞ –∫–æ–º–∞–Ω–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç—É —á–µ—Ä–µ–∑ —Å–æ–∫–µ—Ç
    func handleVoiceCommand(with direction: [String: Bool]) {
        commandSender.moveForward(isPressed: direction["forward"] ?? false)
        commandSender.moveBackward(isPressed: direction["backward"] ?? false)
        commandSender.turnLeft(isPressed: direction["left"] ?? false)
        commandSender.turnRight(isPressed: direction["right"] ?? false)
    }
    
    
    // –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –ø–æ—Å–ª–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã
    func startRepeatingLastCommand() {
        stopRepeatingCommand() // –°–Ω–∞—á–∞–ª–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ –±—ã–ª –∑–∞–ø—É—â–µ–Ω
        repeatCommandTimer = Timer.scheduledTimer(withTimeInterval: 0.5, repeats: true) { [weak self] _ in
            guard let self = self, let direction = self.lastDirection else { return }
            self.handleVoiceCommand(with: direction)
            self.logger.info("üîÅ –ü–æ–≤—Ç–æ—Ä –∫–æ–º–∞–Ω–¥—ã: \(direction)")
        }
    }

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä
    func stopRepeatingCommand() {
        self.logger.info("‚úÖ –¢–∞–π–º–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        repeatCommandTimer?.invalidate()
        repeatCommandTimer = nil
    }
    
    
//    func stopSpeaking() {
//        synthesizer.stopSpeaking(at: .immediate)
//        isSpeaking = false
//    }
    
    
    // –º–µ—Ç–æ–¥ –æ–∑–≤—É—á–∫–∏
    func speak(text: String, language: String = AppConfig.VoiceService.language, rate: Float = 0.5, completion: (() -> Void)? = nil) {
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: language)
        utterance.rate = rate
        //stopAudioEngine()
        isSpeaking = true
        synthesizer.delegate = self
        speechCompletion = completion
        synthesizer.speak(utterance)
    }

    private var speechCompletion: (() -> Void)?

}

// AVSpeechSynthesizerDelegate
extension VoiceService: AVSpeechSynthesizerDelegate {
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        DispatchQueue.main.async {
            self.isSpeaking = false
            self.speechCompletion?()
            self.speechCompletion = nil
        }
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didStart utterance: AVSpeechUtterance) {
        logger.info("üó£ –ù–∞—á–∞–ª–æ –æ–∑–≤—É—á–∫–∏: \(utterance.speechString)")
    }

    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didCancel utterance: AVSpeechUtterance) {
        logger.info("‚õîÔ∏è –û–∑–≤—É—á–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞: \(utterance.speechString)")
    }
}
