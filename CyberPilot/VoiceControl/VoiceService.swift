//
//  VoiceControlManager.swift
//  CyberPilot
//
//  Created by Aleksandr Chumakov on 1/07/25.
//
import Foundation
import AVFoundation
import Speech
import Combine


final class VoiceService: NSObject, ObservableObject {
    let commandSender: CommandSender
    let logger = CustomLogger(logLevel: .info, includeMetadata: false)
    private var clearTimer: Timer?
    @Published var transcribedText: String = ""
    @Published var isListening: Bool = false
    @Published var isSpeaking: Bool = false


    private var speechRecognizer: SFSpeechRecognizer?
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let synthesizer = AVSpeechSynthesizer()
    
    var silenceTimer: Timer?
    

    init(commandSender: CommandSender) {
            self.commandSender = commandSender
            super.init()
            NotificationCenter.default.addObserver(
                    self,
                    selector: #selector(handleAudioSessionInterruption),
                    name: AVAudioSession.interruptionNotification,
                    object: nil
                )
        }
    
    
    @objc private func handleAudioSessionInterruption(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }

        if type == .began {
            logger.info("üîá –ê—É–¥–∏–æ—Å–µ—Å—Å–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞")
            stopListening()
        }
    }

    
    func restartSilenceTimer() {
        silenceTimer?.invalidate()
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 60.0, repeats: false) { [weak self] _ in
            self?.logger.info("‚èπ –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: —Ç–∏—à–∏–Ω–∞")
            self?.stopListening()
        }
    }
    
    func requestAuthorization() {
        
        if speechRecognizer?.supportsOnDeviceRecognition == true {
            recognitionRequest?.requiresOnDeviceRecognition = true
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ")
        } else {
            logger.info("–õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
        }
        // –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        self.speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: AppConfig.VoiceService.language))
            if speechRecognizer == nil {
                logger.info("‚õîÔ∏è –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
                return
            }
        // —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    self.logger.info("‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ.")
                default:
                    self.logger.info("‚õîÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ: \(authStatus)")
                }
            }
        }
        // –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É
        if #available(iOS 17.0, *) {
            AVAudioApplication.requestRecordPermission { granted in
                if !granted {
                    self.logger.info("–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω")
                }
            }
        } else {
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                if !granted {
                    self.logger.info("–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω")
                }
            }
        }
    }
    
    
    // –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ–ª–æ—Å–æ–≤ ru-RU - –º–∏–ª–µ–Ω–∞
    func checkVoices() {
        let voices = AVSpeechSynthesisVoice.speechVoices()
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –≥–æ–ª–æ—Å–∞: \(voices)")
    }
    
    
    // –û—Å—Ç–∞–Ω–∞–≤–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á –∞—É–¥–∏–æ–¥–≤–∏–∂–∫–∞
    func stopAudioEngine() {
        audioEngine.stop()
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        do {
            try AVAudioSession.sharedInstance().setActive(false)
        } catch {
            logger.info("‚õîÔ∏è –û—à–∏–±–∫–∞ –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏: \(error.localizedDescription)")
        }
    }
    
    
    // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏
    func settingAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord,
                                         mode: .voiceChat,
                                         options: [.duckOthers, .defaultToSpeaker, .allowBluetooth])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            logger.info("‚úÖ –ê—É–¥–∏–æ—Å–µ—Å—Å–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
        } catch {
            logger.info("‚õîÔ∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏: \(error.localizedDescription)")
            return
        }
    }
    
    
    // –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ–¥–≤–∏–∂–æ–∫
    func startAudioEngine() {
        do {
            audioEngine.prepare()
            try audioEngine.start()
            logger.info("‚úÖ AudioEngine –∑–∞–ø—É—â–µ–Ω, –º–∏–∫—Ä–æ—Ñ–æ–Ω –∞–∫—Ç–∏–≤–µ–Ω")
        } catch {
            logger.info("‚õîÔ∏è –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ audioEngine: \(error.localizedDescription)")
            return
        }
    }

    
    func startListening() {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        guard let speechRecognizer = speechRecognizer else {
            logger.info("‚õîÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –¥–ª—è \(AppConfig.VoiceService.language) –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
            return
        }
        guard !audioEngine.isRunning else {
            logger.info("‚ö†Ô∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ")
            return
        }
        isListening = true
        stopAudioEngine()
        settingAudioSession()
        // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            logger.info("‚õîÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å recognitionRequest")
            return
        }
        
        // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω–Ω–æ–≥–æ –≤—Ö–æ–¥–∞
        let inputNode = audioEngine.inputNode
        inputNode.removeTap(onBus: 0) // –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π tap, –µ—Å–ª–∏ –±—ã–ª
        
        
        //let inputFormat = inputNode.outputFormat(forBus: 0) // –¥–ª—è —Ç–µ—Å—Ç–æ–≤ –≤ —ç–º—É–ª—è—Ç–æ—Ä–µ
        
        // –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–∞
        let sampleRate = AVAudioSession.sharedInstance().sampleRate
        let inputFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                                        sampleRate: sampleRate,
                                        channels: 1,
                                        interleaved: false)!
        
        
        logger.info("–§–æ—Ä–º–∞—Ç –∞—É–¥–∏–æ: sampleRate=\(inputFormat.sampleRate), channels=\(inputFormat.channelCount)")
        inputNode.installTap(onBus: 0, bufferSize: 2048, format: inputFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        startAudioEngine()
        // –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –±—É—Ñ–µ—Ä
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }

            if let error = error {
                self.logger.info("‚õîÔ∏è –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: \(error.localizedDescription)")
                self.stopListening()
                return
            }

            guard let result = result else { return }

            let text = result.bestTranscription.formattedString
            self.transcribedText = text
            self.restartSilenceTimer()

            let words = text.lowercased().components(separatedBy: " ").filter { !$0.isEmpty }
            //let lastWords = words.suffix(2).joined(separator: " ")  // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1‚Äì2 —Å–ª–æ–≤–∞
            let lastWord = words.last ?? "" // –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ

            if let voiceCommand = VoiceCommand.parse(from: lastWord) {
                let direction = self.commandDirection(from: voiceCommand)
                self.handleVoiceCommand(with: direction)
                self.logger.info("‚úÖ –ö–æ–º–∞–Ω–¥–∞: \(direction)")
                self.logger.info("üì¢ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: \(lastWord)")
                self.transcribedText = ""
            } else {
                self.logger.info("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: \(lastWord)")
            }

            if result.isFinal {
                self.logger.info("‚èπ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                self.stopListening()
            }
        }
    }
    
    func commandDirection(from command: VoiceCommand?) -> [String: Bool] {
        var flags = [
            "forward": false,
            "backward": false,
            "left": false,
            "right": false
        ]
        if let cmd = command {
            switch cmd {
            case .forward:
                flags["forward"] = true
            case .backward:
                flags["backward"] = true
            case .left:
                flags["left"] = true
            case .right:
                flags["right"] = true
            case .forwardLeft:
                flags["forward"] = true
                flags["left"] = true
            case .forwardRight:
                flags["forward"] = true
                flags["right"] = true
            case .stop:
                break
            }
        }

        return flags
    }


    
    func handleVoiceCommand(with direction: [String: Bool]) {
        commandSender.moveForward(isPressed: direction["forward"] ?? false)
        commandSender.moveBackward(isPressed: direction["backward"] ?? false)
        commandSender.turnLeft(isPressed: direction["left"] ?? false)
        commandSender.turnRight(isPressed: direction["right"] ?? false)
    }
    
    
    // –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    func stopListening() {
        stopAudioEngine()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest = nil
        recognitionTask = nil
        isListening = false
    }

    
    // –º–µ—Ç–æ–¥ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≥–æ–ª–æ—Å–æ–≤—É—é —Ä–µ—á—å
    func speak(text: String, language: String = AppConfig.VoiceService.language, rate: Float = 0.5) {
        stopListening()

        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: language)
        utterance.rate = rate

        isSpeaking = true
        synthesizer.delegate = self
        synthesizer.speak(utterance)
    }

    
    func stopSpeaking() {
        synthesizer.stopSpeaking(at: .immediate)
        isSpeaking = false
    }
}

// AVSpeechSynthesizerDelegate
extension VoiceService: AVSpeechSynthesizerDelegate {
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        DispatchQueue.main.async {
            self.isSpeaking = false
        }
    }
}
